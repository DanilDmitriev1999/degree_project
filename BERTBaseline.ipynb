{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTBaseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d68824e554874ee8a7a05b5041aeba11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_43f952a884f648bf96f7d570ec674681",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d47cb394eedf4c2e938ef2fc953dfcd5",
              "IPY_MODEL_1c4ac02632764f3a9ee970db672f98e9"
            ]
          }
        },
        "43f952a884f648bf96f7d570ec674681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d47cb394eedf4c2e938ef2fc953dfcd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a1960af28a645658da60458188b9eba",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_954c7915ef6f49a0b44d7d618f7fca52"
          }
        },
        "1c4ac02632764f3a9ee970db672f98e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a6ef28d16f7a498bac8b3653666e96e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 1.91MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84a717e7ec7b4ee3aeeb8a8551751745"
          }
        },
        "9a1960af28a645658da60458188b9eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "954c7915ef6f49a0b44d7d618f7fca52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6ef28d16f7a498bac8b3653666e96e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84a717e7ec7b4ee3aeeb8a8551751745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hduqgFLrrM5W",
        "colab_type": "text"
      },
      "source": [
        "# Техническая часть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFPkkDFbrIRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/DanilDmitriev1999/degree-project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMpcfBp9r6XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCQcJ-BrR03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import AdamW, BertModel\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, TensorDataset\n",
        "\n",
        "#import fire\n",
        "\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS8Ul-8QttKF",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5INR1CoWuI2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_tag(chunk_tag):\n",
        "    \"\"\"\n",
        "    split chunk tag into IOBES prefix and chunk_type\n",
        "    e.g. \n",
        "    B-PER -> (B, PER)\n",
        "    O -> (O, None)\n",
        "    \"\"\"\n",
        "    if chunk_tag == 'O':\n",
        "        return ('O', None)\n",
        "    return chunk_tag.split('-', maxsplit=1)\n",
        "\n",
        "def is_chunk_end(prev_tag, tag):\n",
        "    \"\"\"\n",
        "    check if the previous chunk ended between the previous and current word\n",
        "    e.g. \n",
        "    (B-PER, I-PER) -> False\n",
        "    (B-LOC, O)  -> True\n",
        "    Note: in case of contradicting tags, e.g. (B-PER, I-LOC)\n",
        "    this is considered as (B-PER, B-LOC)\n",
        "    \"\"\"\n",
        "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
        "    prefix2, chunk_type2 = split_tag(tag)\n",
        "\n",
        "    if prefix1 == 'O':\n",
        "        return False\n",
        "    if prefix2 == 'O':\n",
        "        return prefix1 != 'O'\n",
        "\n",
        "    if chunk_type1 != chunk_type2:\n",
        "        return True\n",
        "\n",
        "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']\n",
        "\n",
        "def is_chunk_start(prev_tag, tag):\n",
        "    \"\"\"\n",
        "    check if a new chunk started between the previous and current word\n",
        "    \"\"\"\n",
        "    prefix1, chunk_type1 = split_tag(prev_tag)\n",
        "    prefix2, chunk_type2 = split_tag(tag)\n",
        "\n",
        "    if prefix2 == 'O':\n",
        "        return False\n",
        "    if prefix1 == 'O':\n",
        "        return prefix2 != 'O'\n",
        "\n",
        "    if chunk_type1 != chunk_type2:\n",
        "        return True\n",
        "\n",
        "    return prefix2 in ['B', 'S'] or prefix1 in ['E', 'S']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGlQSq8euFBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_metrics(tp, p, t, percent=True):\n",
        "    \"\"\"\n",
        "    compute overall precision, recall and FB1 (default values are 0.0)\n",
        "    if percent is True, return 100 * original decimal value\n",
        "    \"\"\"\n",
        "    precision = tp / p if p else 0\n",
        "    recall = tp / t if t else 0\n",
        "    fb1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n",
        "    if percent:\n",
        "        return 100 * precision, 100 * recall, 100 * fb1\n",
        "    else:\n",
        "        return precision, recall, fb1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyC--6L-uCHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_chunks(true_seqs, pred_seqs):\n",
        "    \"\"\"\n",
        "    true_seqs: a list of true tags\n",
        "    pred_seqs: a list of predicted tags\n",
        "    return: \n",
        "    correct_chunks: a dict (counter), \n",
        "                    key = chunk types, \n",
        "                    value = number of correctly identified chunks per type\n",
        "    true_chunks:    a dict, number of true chunks per type\n",
        "    pred_chunks:    a dict, number of identified chunks per type\n",
        "    correct_counts, true_counts, pred_counts: similar to above, but for tags\n",
        "    \"\"\"\n",
        "    correct_chunks = defaultdict(int)\n",
        "    true_chunks = defaultdict(int)\n",
        "    pred_chunks = defaultdict(int)\n",
        "\n",
        "    correct_counts = defaultdict(int)\n",
        "    true_counts = defaultdict(int)\n",
        "    pred_counts = defaultdict(int)\n",
        "\n",
        "    prev_true_tag, prev_pred_tag = 'O', 'O'\n",
        "    correct_chunk = None\n",
        "\n",
        "    for true_tag, pred_tag in zip(true_seqs, pred_seqs):\n",
        "        if true_tag == pred_tag:\n",
        "            correct_counts[true_tag] += 1\n",
        "        true_counts[true_tag] += 1\n",
        "        pred_counts[pred_tag] += 1\n",
        "\n",
        "        _, true_type = split_tag(true_tag)\n",
        "        _, pred_type = split_tag(pred_tag)\n",
        "\n",
        "        if correct_chunk is not None:\n",
        "            true_end = is_chunk_end(prev_true_tag, true_tag)\n",
        "            pred_end = is_chunk_end(prev_pred_tag, pred_tag)\n",
        "\n",
        "            if pred_end and true_end:\n",
        "                correct_chunks[correct_chunk] += 1\n",
        "                correct_chunk = None\n",
        "            elif pred_end != true_end or true_type != pred_type:\n",
        "                correct_chunk = None\n",
        "\n",
        "        true_start = is_chunk_start(prev_true_tag, true_tag)\n",
        "        pred_start = is_chunk_start(prev_pred_tag, pred_tag)\n",
        "\n",
        "        if true_start and pred_start and true_type == pred_type:\n",
        "            correct_chunk = true_type\n",
        "        if true_start:\n",
        "            true_chunks[true_type] += 1\n",
        "        if pred_start:\n",
        "            pred_chunks[pred_type] += 1\n",
        "\n",
        "        prev_true_tag, prev_pred_tag = true_tag, pred_tag\n",
        "    if correct_chunk is not None:\n",
        "        correct_chunks[correct_chunk] += 1\n",
        "\n",
        "    return (correct_chunks, true_chunks, pred_chunks, \n",
        "        correct_counts, true_counts, pred_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FziYV4JZt5eI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_result(correct_chunks, true_chunks, pred_chunks,\n",
        "    correct_counts, true_counts, pred_counts, verbose=True):\n",
        "    \"\"\"\n",
        "    if verbose, print overall performance, as well as preformance per chunk type;\n",
        "    otherwise, simply return overall prec, rec, f1 scores\n",
        "    \"\"\"\n",
        "    # sum counts\n",
        "    sum_correct_chunks = sum(correct_chunks.values())\n",
        "    sum_true_chunks = sum(true_chunks.values())\n",
        "    sum_pred_chunks = sum(pred_chunks.values())\n",
        "\n",
        "    sum_correct_counts = sum(correct_counts.values())\n",
        "    sum_true_counts = sum(true_counts.values())\n",
        "\n",
        "    nonO_correct_counts = sum(v for k, v in correct_counts.items() if k != 'O')\n",
        "    nonO_true_counts = sum(v for k, v in true_counts.items() if k != 'O')\n",
        "\n",
        "    chunk_types = sorted(list(set(list(true_chunks) + list(pred_chunks))))\n",
        "\n",
        "    # compute overall precision, recall and FB1 (default values are 0.0)\n",
        "    prec, rec, f1 = calc_metrics(sum_correct_chunks, sum_pred_chunks, sum_true_chunks)\n",
        "    res = (prec, rec, f1)\n",
        "    if not verbose:\n",
        "        return res\n",
        "\n",
        "    # print overall performance, and performance per chunk type\n",
        "    \n",
        "    print(\"processed %i tokens with %i phrases; \" % (sum_true_counts, sum_true_chunks), end='')\n",
        "    print(\"found: %i phrases; correct: %i.\\n\" % (sum_pred_chunks, sum_correct_chunks), end='')\n",
        "        \n",
        "    print(\"accuracy: %6.2f%%; (non-O)\" % (100*nonO_correct_counts/nonO_true_counts))\n",
        "    print(\"accuracy: %6.2f%%; \" % (100*sum_correct_counts/sum_true_counts), end='')\n",
        "    print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" % (prec, rec, f1))\n",
        "\n",
        "    # for each chunk type, compute precision, recall and FB1 (default values are 0.0)\n",
        "    for t in chunk_types:\n",
        "        prec, rec, f1 = calc_metrics(correct_chunks[t], pred_chunks[t], true_chunks[t])\n",
        "        print(\"%17s: \" %t , end='')\n",
        "        print(\"precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f\" %\n",
        "                    (prec, rec, f1), end='')\n",
        "        print(\"  %d\" % pred_chunks[t])\n",
        "\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg2zKuUBr3Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(true_seqs, pred_seqs, verbose=True):\n",
        "    (correct_chunks, true_chunks, pred_chunks,\n",
        "        correct_counts, true_counts, pred_counts) = count_chunks(true_seqs, pred_seqs)\n",
        "    result = get_result(correct_chunks, true_chunks, pred_chunks,\n",
        "        correct_counts, true_counts, pred_counts, verbose=verbose)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CYVYGn5t7JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertNER(pl.LightningModule):\n",
        "    def __init__(self, num_labels, lr, train_dataloader, val_dataloader, ids_to_labels, labels_to_ids):\n",
        "        super(BertNER, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.lr = lr\n",
        "\n",
        "        self.model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.classifier = torch.nn.Linear(768, num_labels) # model cant not output \"X\" labels\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "        self.traindl, self.valdl = train_dataloader, val_dataloader # we can't overwrite self.train, self.train_dataloader\n",
        "        self.ids_to_labels = ids_to_labels\n",
        "        self.labels_to_ids = labels_to_ids\n",
        "\n",
        "    def f1(self, y_true, y_pred):\n",
        "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "        y_true = flatten(y_true)\n",
        "        y_pred = flatten(y_pred)\n",
        "        y_true = [self.ids_to_labels[l] for l in y_true]\n",
        "        y_pred = [self.ids_to_labels[l] for l in y_pred]\n",
        "        assert len(y_pred) == len(y_true)\n",
        "\n",
        "        ids = [i for i, label in enumerate(y_true) if label != \"X\"]\n",
        "        y_true_cleaned = [y_true[i] for i in ids]\n",
        "        y_pred_cleaned = [y_pred[i] for i in ids]\n",
        "\n",
        "        precision, recall, f1 = evaluate(y_true_cleaned, y_pred_cleaned)#, verbose=False)\n",
        "        print(f\"micro average precision: {precision}, recall: {recall}, f1: {f1}\")\n",
        "        return precision, recall, f1\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        sequence_output = self.model.forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,)\n",
        "        if labels is not None:\n",
        "            # reference:\n",
        "            # https://github.com/huggingface/transformers/blob/d5d7d886128732091e92afff7fcb3e094c71a7ec/src/transformers/modeling_bert.py#L1380-L1394\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            # X label described in Bert Paper section 4.3\n",
        "            X = self.labels_to_ids[\"X\"]\n",
        "            not_X_mask = labels != X # since label of PAD is \"X\", attention mask is not needed\n",
        "\n",
        "            # Only keep active parts of the loss\n",
        "            active_loss = not_X_mask.view(-1)\n",
        "            active_logits = logits.view(-1, self.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, score = self.forward(*batch)\n",
        "        tqdm_dict = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, mask, labels = batch\n",
        "        loss, score = self.forward(*batch)\n",
        "        labels_pred = torch.argmax(score, dim=-1)\n",
        "        return {\n",
        "            \"val_loss\": loss,\n",
        "            \"y_true\": labels,\n",
        "            \"y_pred\": labels_pred,\n",
        "            \"mask\": mask,\n",
        "        }\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
        "\n",
        "        y_true, y_pred = [], []\n",
        "        for out in outputs:\n",
        "            batch_y_true = out[\"y_true\"].cpu().numpy().tolist()\n",
        "            batch_y_pred = out[\"y_pred\"].cpu().numpy().tolist()\n",
        "            batch_seq_lens = out[\"mask\"].cpu().numpy().sum(-1).tolist()\n",
        "            for i, length in enumerate(batch_seq_lens):\n",
        "                batch_y_true[i] = batch_y_true[i][:length]\n",
        "                batch_y_pred[i] = batch_y_pred[i][:length]\n",
        "            y_true += batch_y_true\n",
        "            y_pred += batch_y_pred\n",
        "\n",
        "        precision, recall, f1 = self.f1(y_true, y_pred)\n",
        "        tqdm_dict = {\n",
        "            \"val_loss\": val_loss,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
        "        return result\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return self.traindl\n",
        "\n",
        "    @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return self.valdl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx1OOyxbu1Qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_tokens_to_ids(tokens, pad=True):\n",
        "    \"\"\"Helper function\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ids = torch.LongTensor([token_ids])\n",
        "    assert ids.size(1) < max_len, print(ids.size(1))\n",
        "    if pad:\n",
        "        padded_ids = torch.zeros(max_len).long()\n",
        "        padded_ids[:ids.size(1)] = ids\n",
        "        mask = torch.zeros(max_len).long()\n",
        "        mask[0:ids.size(1)] = 1\n",
        "        return padded_ids, mask\n",
        "    else:\n",
        "        return ids\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv9ylaATu13Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subword_tokenize(tokens, labels):\n",
        "    \"\"\"\n",
        "    Helper function\n",
        "    Segment each token into subwords while keeping track of\n",
        "    token boundaries.\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens: A sequence of strings, representing input tokens.\n",
        "    Returns\n",
        "    -------\n",
        "    A tuple consisting of:\n",
        "        - A list of subwords, flanked by the special symbols required\n",
        "            by Bert (CLS and SEP).\n",
        "        - An array of indices into the list of subwords, indicating\n",
        "          that the corresponding subword is the start of a new\n",
        "            token. For example, [1, 3, 4, 7] means that the subwords\n",
        "            1, 3, 4, 7 are token starts, while all other subwords\n",
        "            (0, 2, 5, 6, 8...) are in or at the end of tokens.\n",
        "            This list allows selecting Bert hidden states that\n",
        "            represent tokens, which is necessary in sequence\n",
        "            labeling.\n",
        "    \"\"\"\n",
        "    def flatten(list_of_lists):\n",
        "        for list in list_of_lists:\n",
        "            for item in list:\n",
        "                yield item\n",
        "\n",
        "    subwords = list(map(tokenizer.tokenize, tokens))\n",
        "    subword_lengths = list(map(len, subwords))\n",
        "    subwords = [CLS] + list(flatten(subwords)) + [SEP]\n",
        "    token_start_idxs = 1 + np.cumsum([0] + subword_lengths[:-1])\n",
        "    # X label described in Bert Paper section 4.3\n",
        "    bert_labels = [[label] + (sublen-1) * [\"X\"] for sublen, label in zip(subword_lengths, labels)]\n",
        "    bert_labels = [\"O\"] + list(flatten(bert_labels)) + [\"O\"]\n",
        "\n",
        "    assert len(subwords) == len(bert_labels)\n",
        "    assert len(subwords) <= 512\n",
        "    return subwords, token_start_idxs, bert_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUTZ9U0au-28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subword_tokenize_to_ids(tokens, labels):\n",
        "    \"\"\"Segment each token into subwords while keeping track of token boundaries and convert subwords into IDs.\n",
        "    Parameters\n",
        "        ----------\n",
        "        tokens: A sequence of strings, representing input tokens.\n",
        "        Returns\n",
        "        -------\n",
        "        A tuple consisting of:\n",
        "            - A list of subword IDs, including IDs of the special\n",
        "                symbols (CLS and SEP) required by Bert.\n",
        "            - A mask indicating padding tokens.\n",
        "            - An array of indices into the list of subwords. See\n",
        "                doc of subword_tokenize.\n",
        "    \"\"\"\n",
        "    assert len(tokens) == len(labels)\n",
        "    subwords, token_start_idxs, bert_labels = subword_tokenize(tokens, labels)\n",
        "    subword_ids, mask = convert_tokens_to_ids(subwords)\n",
        "    token_starts = torch.zeros(max_len)\n",
        "    token_starts[token_start_idxs] = 1\n",
        "    bert_labels = [labels_to_ids[label] for label in bert_labels]\n",
        "    # X label described in Bert Paper section 4.3 is used for pading\n",
        "    padded_bert_labels = torch.ones(max_len).long() * labels_to_ids[\"X\"]\n",
        "    padded_bert_labels[:len(bert_labels)] = torch.LongTensor(bert_labels)\n",
        "\n",
        "    mask.require_grad = False\n",
        "    return {\n",
        "        \"input_ids\": subword_ids,\n",
        "        \"attention_mask\": mask,\n",
        "        \"bert_token_starts\": token_starts,\n",
        "        \"labels\": padded_bert_labels\n",
        "    }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGOXSbaEuckH",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rZyoYctuaNU",
        "colab_type": "code",
        "outputId": "e465a8eb-126d-47f8-94b0-b056dea1d692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d68824e554874ee8a7a05b5041aeba11",
            "43f952a884f648bf96f7d570ec674681",
            "d47cb394eedf4c2e938ef2fc953dfcd5",
            "1c4ac02632764f3a9ee970db672f98e9",
            "9a1960af28a645658da60458188b9eba",
            "954c7915ef6f49a0b44d7d618f7fca52",
            "a6ef28d16f7a498bac8b3653666e96e5",
            "84a717e7ec7b4ee3aeeb8a8551751745"
          ]
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "SEP = \"[SEP]\"\n",
        "MASK = \"[MASK]\"\n",
        "CLS = \"[CLS]\"\n",
        "max_len = 402"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d68824e554874ee8a7a05b5041aeba11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVq0kqjGuh2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS = ['O', 'B-Person', 'I-Person', 'B-Org', 'I-Org', 'B-Loc', 'I-Loc', \"X\"]\n",
        "ids_to_labels = {k:v for k, v in enumerate(LABELS)}\n",
        "labels_to_ids = {v:k for k, v in enumerate(LABELS)}\n",
        "num_labels = len(LABELS) - 1 # model can't output \"X\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjWrMdX2wPJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CoNLL(Dataset):\n",
        "    \"\"\"simple class to read raw dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, path=\"degree-project/data\"):\n",
        "        entries = open(path, \"r\").read().strip().split(\"\\n\\n\")\n",
        "\n",
        "        self.sentence, self.label = [], []  # list of lists\n",
        "        for entry in entries:\n",
        "            words = [line.split()[0] for line in entry.splitlines()]\n",
        "            tags = [self._check(line.split()[-1]) for line in entry.splitlines()]\n",
        "            self.sentence.append(words)\n",
        "            self.label.append(tags)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    @staticmethod\n",
        "    def _check(tag):\n",
        "        if tag == 'B-Location':\n",
        "            tag = 'B-Loc'\n",
        "        if tag == 'I-Location':\n",
        "            tag = 'I-Loc'\n",
        "        if tag == 'I-Facility':\n",
        "            tag = 'O'\n",
        "        if tag == 'I-LocOrg':\n",
        "            tag = 'I-Loc'\n",
        "        if tag == 'B-LocOrg':\n",
        "            tag = 'B-Loc' \n",
        "        return tag\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.sentence[i], self.label[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zGnGEFQvSSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_dataset(path='degree-project/data/dev.txt'):\n",
        "    dataset = CoNLL(path)\n",
        "    featurized_sentences = []\n",
        "    for tokens, labels in dataset:\n",
        "        features = subword_tokenize_to_ids(tokens, labels)\n",
        "        featurized_sentences.append(features)\n",
        "\n",
        "    def collate(featurized_sentences_batch):\n",
        "        keys = (\"input_ids\", \"attention_mask\", \"bert_token_starts\", \"labels\")\n",
        "        output = {key: torch.stack([fs[key] for fs in featurized_sentences_batch], dim=0) for key in keys}\n",
        "        return output\n",
        "\n",
        "    dataset = collate(featurized_sentences)\n",
        "    return TensorDataset(*[dataset[k] for k in (\"input_ids\", \"attention_mask\", \"labels\")])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKTMEGfuBcvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = prepare_dataset('degree-project/data/dev.txt')\n",
        "train_dataset = prepare_dataset('degree-project/data/train.txt')\n",
        "batch_size=16\n",
        "\n",
        "sampler = RandomSampler(train_dataset)\n",
        "train_dataloader= DataLoader(train_dataset, sampler=sampler, batch_size=batch_size, pin_memory=True)\n",
        "val_dataloader= DataLoader(val_dataset, batch_size=batch_size, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8XPR8hS0TGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size=16\n",
        "lr=5e-5\n",
        "epoch=6\n",
        "N_GPUs = torch.cuda.device_count()\n",
        "model = BertNER(num_labels, lr, train_dataloader, val_dataloader, ids_to_labels, labels_to_ids).to(device)\n",
        "trainer = pl.Trainer(\n",
        "        fast_dev_run=False if N_GPUs > 0 else True,\n",
        "        gpus=N_GPUs if N_GPUs != 0 else 0,\n",
        "        distributed_backend=\"dp\" if N_GPUs > 1 else None,\n",
        "        max_epochs=epoch,\n",
        "        #overfit_pct=0.01\n",
        ")\n",
        "trainer.fit(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akxlVh5_v1EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}